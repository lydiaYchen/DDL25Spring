{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(rank: int, world_size: int) -> None:\n",
    "    from simplellm.llama import CausalLLama, LLama # get our models\n",
    "    from simplellm.tokenizers import SPTokenizer # get our tokenizer\n",
    "    from simplellm.dataloaders import TinyStories # get our dataset\n",
    "    from simplellm.losses import causalLLMLoss # our loss\n",
    "    from torch.optim import SGD, Adam\n",
    "    import torch.nn.functional as F\n",
    "    import torch\n",
    "    import torch.distributed as dist\n",
    "    import os\n",
    "    import contextlib\n",
    "\n",
    "    with open(f\"out{rank}.txt\", \"w\", buffering=1) as f, contextlib.redirect_stdout(f):\n",
    "        os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "        os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "        dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "        torch.manual_seed(0)\n",
    "        dmodel = 288\n",
    "        num_heads = 6\n",
    "        n_layers = 6\n",
    "        seq_l = 256\n",
    "        batch_size = 1\n",
    "        device = \"cuda\"\n",
    "\n",
    "        # make the tokenizer\n",
    "        tokenizer = SPTokenizer()\n",
    "        # make the model\n",
    "        net = LLama(CausalLLama,tokenizer.vocab_size,dmodel=dmodel,num_heads=num_heads,\n",
    "                        device=device, n_layers=n_layers, ctx_size=seq_l,padding_idx=tokenizer.pad_id)\n",
    "        ds = TinyStories(tokenizer,batch_size=batch_size, seq_l=seq_l,skip=rank*5000) # skip so we can have different things\n",
    "        # we can iterate the dataset with:\n",
    "        iter_ds = iter(ds)\n",
    "\n",
    "        optim = Adam(net.parameters(),lr=8e-4)\n",
    "\n",
    "        sizes = []\n",
    "        len_sizes = []\n",
    "        for param in net.parameters():\n",
    "            sizes.append(param.shape)\n",
    "            len_sizes.append(len(param.view(-1)))\n",
    "\n",
    "        for itr in range(2_000):\n",
    "            optim.zero_grad()\n",
    "            x = next(iter_ds)\n",
    "            target = x.clone().detach()\n",
    "            x = x.to(device)\n",
    "\n",
    "            x = net(x)\n",
    "            loss = causalLLMLoss(x,target,tokenizer.vocab_size)\n",
    "            # log the loss:\n",
    "            print(itr,loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            dist.barrier() # wait for everyone\n",
    "\n",
    "            tmp = []\n",
    "            for param in net.parameters():\n",
    "                if param.grad == None:\n",
    "                    tmp.append(torch.zeros_like(param,device=\"cpu\").view(-1))\n",
    "                    continue\n",
    "                tmp.append(param.grad.view(-1))\n",
    "                param.grad = None\n",
    "            prev_grad = torch.cat(tmp).to(\"cpu\")\n",
    "            dist.all_reduce(prev_grad, op = dist.ReduceOp.SUM)\n",
    "            tmp = torch.split(prev_grad, len_sizes)\n",
    "            for i, param in enumerate(net.parameters()):\n",
    "                param.grad = tmp[i].view(sizes[i]).to(device)/world_size # average\n",
    "            optim.step()\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing.context import ForkProcess\n",
    "\n",
    "world_size = 3\n",
    "ctx = multiprocessing.get_context(\"fork\")\n",
    "processes: list[ForkProcess] = []\n",
    "\n",
    "for rank in range(world_size):\n",
    "    p = ctx.Process(target=run, args=(rank, world_size))\n",
    "    processes.append(p)\n",
    "    p.start()\n",
    "\n",
    "for p in processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
